version: '3'
services:

  elasticsearch:
    image: elasticsearch:7.3.1
    environment:
      - cluster.name=combine-elasticsearch
      - ES_JAVA_OPTS=-Xms1024m -Xmx1024m
      - discovery.type=single-node
      - http.max_initial_line_length=5m
      - path.repo=/tmp/es_repo
      - xpack.security.enabled=false
    volumes:
      - esdata:/usr/share/elasticsearch/data
    healthcheck:
        test: ["CMD-SHELL", "curl --silent --fail localhost:9200/_cluster/health || exit 1"]
        interval: 30s
        timeout: 30s
        retries: 3
    ports:
      - 9200:9200
    networks:
      combinenet:
        ipv4_address: 10.5.0.2

  mysql:
    image: mysql:8
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_PASSWORD}
    volumes:
      - mysqldata:/var/lib/mysql
      - ./mysql:/etc/mysql/conf.d
    healthcheck:
        test: ["CMD-SHELL", "mysqladmin -h 'localhost' -u root -pWelcome123 ping --silent"]
        interval: 30s
        timeout: 30s
        retries: 3
    command:
      - '--default-authentication-plugin=mysql_native_password'
    networks:
      combinenet:
        ipv4_address: 10.5.0.4

  mongo:
    image: mongo:4
    volumes:
      - mongodata:/data/db
      - ./mongo/mongod.conf:/etc/mongod.conf
    ports:
      - 27018:27017
    command:
      - '--config'
      - '/etc/mongod.conf'
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongo mongo:27017/test --quiet 1
      interval: 10s
      timeout: 10s
      retries: 5
    networks:
      combinenet:
        ipv4_address: 10.5.0.3

  redis:
    image: redis:5
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      combinenet:
        ipv4_address: 10.5.0.5

  spark-cluster-base:
    build:
      context: ./spark_cluster_base
      dockerfile: Dockerfile
      args:
        SPARK_VERSION: "${SPARK_VERSION}"
        HADOOP_VERSION_SHORT: "${HADOOP_VERSION_SHORT}"
        ELASTICSEARCH_HADOOP_CONNECTOR_VERSION: "${ELASTICSEARCH_HADOOP_CONNECTOR_VERSION}"

  hadoop-namenode:
    build:
      context: ./hadoop
      dockerfile: Dockerfile
      args:
        HADOOP_VERSION: "${HADOOP_VERSION}"
    volumes:
      - ./combinelib:/combinelib
      - hdfs:/hdfs
      - hadoop_binaries:/opt/hadoop
    ports:
      - 8020:8020
    command:
      - '/opt/hadoop/bin/hdfs'
      - '--config'
      - '/opt/hadoop/etc/hadoop'
      - 'namenode'
    depends_on:
      - spark-cluster-base
    networks:
      combinenet:
        ipv4_address: 10.5.0.6

  hadoop-datanode:
    build:
      context: ./hadoop
      dockerfile: Dockerfile
      args:
        HADOOP_VERSION: "${HADOOP_VERSION}"
    volumes:
      - ./combinelib:/combinelib
    ports:
      - 50070:50070
      - 50075:50075
    command:
      - '/opt/hadoop/bin/hdfs'
      - '--config'
      - '/opt/hadoop/etc/hadoop'
      - 'datanode'
    depends_on:
      - hadoop-namenode
    networks:
      combinenet:
        ipv4_address: 10.5.0.7

  combine-django:
    build:
      context: ./combine
      dockerfile: Dockerfile
      args:
        LIVY_TAGGED_RELEASE: "${LIVY_TAGGED_RELEASE}"
        SCALA_VERSION: "${SCALA_VERSION}"
        COMBINE_BRANCH: "${COMBINE_BRANCH}"
    volumes:
      - ./combinelib:/combinelib
      - ./combine/combine:/opt/combine
      - combine_home:/home/combine
      - combine_python_env:/opt/conda/envs/combine
      - combine_tmp:/tmp
      - hadoop_binaries:/opt/hadoop
      - hdfs:/hdfs
      - livy_binaries:/opt/livy
      - spark_binaries:/opt/spark
    ports:
      - ${COMBINE_DJANGO_HOST_PORT}:8000
    command: bash -c "nohup pyjxslt 6767 & > /tmp/pyjxslt.out && python /opt/combine/manage.py runserver 0.0.0.0:8000"
    depends_on:
      - mysql
      - elasticsearch
    networks:
      combinenet:
        ipv4_address: 10.5.0.10


  # combine-celery:
  #   image: combine-docker_combine-django
  #   volumes:
  #     - ./combinelib:/combinelib
  #     - type: volume
  #       source: hdfs
  #       target: /hdfs
  #       read_only: true
  #     - type: volume
  #       source: hadoop_binaries
  #       target: /opt/hadoop
  #       read_only: true
  #     - type: volume
  #       source: spark_binaries
  #       target: /opt/spark
  #       read_only: true
  #     - type: volume
  #       source: livy_binaries
  #       target: /opt/livy
  #       read_only: true
  #     - type: bind # Bind Combine app submodule
  #       source: ./combine/combine
  #       target: /opt/combine
  #       read_only: false # owner
  #       bind:
  #         propagation: shared
  #     - type: volume
  #       source: combine_home
  #       target: /home/combine
  #       read_only: false # owner
  #     - type: volume
  #       source: combine_python_env
  #       target: /opt/conda/envs/combine
  #       read_only: false # owner
  #     - type: volume
  #       source: combine_tmp
  #       target: /tmp
  #       read_only: false
  #   working_dir: /opt/combine
  #   command:  celery -A core worker -l info --concurrency 1
  #   depends_on:
  #     - redis
  #   networks:
  #     combinenet:
  #       ipv4_address: 10.5.0.12

  # livy:
  #   build:
  #     context: ./livy
  #     dockerfile: Dockerfile
  #     args:
  #       LIVY_TAGGED_RELEASE: "${LIVY_TAGGED_RELEASE}"
  #       SCALA_VERSION: "${SCALA_VERSION}"
  #   volumes:
  #     - ./combinelib:/combinelib
  #     - type: volume
  #       source: hdfs
  #       target: /hdfs
  #       read_only: true
  #     - type: volume
  #       source: hadoop_binaries
  #       target: /opt/hadoop
  #       read_only: true
  #     - type: volume
  #       source: spark_binaries
  #       target: /opt/spark
  #       read_only: false # owner of /opt/spark when Livy only
  #     - type: volume
  #       source: livy_binaries
  #       target: /opt/livy
  #       read_only: false # owner of dir
  #     - type: bind # Bind Combine app submodule
  #       source: ./combine/combine
  #       target: /opt/combine
  #       read_only: false # owner
  #       bind:
  #         propagation: shared
  #     - type: volume
  #       source: combine_home
  #       target: /home/combine
  #       read_only: false
  #     - type: volume
  #       source: combine_python_env
  #       target: /opt/conda/envs/combine
  #       read_only: true
  #     - type: volume
  #       source: combine_tmp
  #       target: /tmp
  #       read_only: false
  #   ports:
  #     - 8998:8998
  #     - 4040:4040
  #   command:  bash -c "nohup pyjxslt 6767 & > /tmp/pyjxslt.out && /opt/livy/bin/livy-server"
  #   networks:
  #     combinenet:
  #       ipv4_address: 10.5.0.11


volumes:
  # non-volatile
  esdata:
    driver: local
  mongodata:
    driver: local
  mysqldata:
    driver: local
  hdfs:
    driver: local
  combine_home:
    driver: local

  # volatile
  combine_python_env:
    driver: local
  hadoop_binaries:
    driver: local
  spark_binaries:
    driver: local
  livy_binaries:
    driver: local
  combine_tmp:
    driver: local


networks:
  combinenet:
    driver: bridge
    ipam:
     driver: default
     config:
       - subnet: 10.5.0.0/16
